{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcEHM2e1yiCWWoAdzvF8Ly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/callmemark/Correlation-of-Night-Air-Temperature-and-light-pollution-in-major-cities-in-the-Philippines/blob/main/Thesis_Proc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Correlation of Night Air Temperature and light pollution in major cities in the Philippines </h1>\n",
        "\n",
        "\n",
        "---\n",
        "</br></br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Fernandez, Jaron Rix\n",
        "</br>\n",
        "Velmonte, Mark john A.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RABrvrp2qX4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Preperation 1: Install Libraries</h1>\n",
        "\n",
        "> Install neccesary libraries"
      ],
      "metadata": {
        "id": "Sy9S6BtAqfyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install GDAL and Geopandas\n",
        "!apt install gdal-bin python-gdal python3-gdal --quiet\n",
        "!apt install python3-rtree --quiet\n",
        "!pip install --upgrade pip --quiet\n",
        "!pip install git+git://github.com/geopandas/geopandas.git --quiet\n",
        "!pip install descartes --quiet\n",
        "!pip install ipywidgets --quiet\n",
        "\n",
        "# Install other libraries\n",
        "!pip install splot --quiet\n",
        "!pip install -U mgwr --quiet\n",
        "\n",
        "#!pip freeze"
      ],
      "metadata": {
        "id": "LdXV1hAsnuyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Preperation 2: Import Libraries</h1>\n",
        "\n",
        "> after installation we procced to import the libraries"
      ],
      "metadata": {
        "id": "YJrBnv2Zqszf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9oawj62ciWQo"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import libpysal as ps\n",
        "from mgwr.gwr import GWR, MGWR\n",
        "from mgwr.sel_bw import Sel_BW\n",
        "from mgwr.utils import shift_colormap, truncate_colormap\n",
        "from google.colab import drive\n",
        "from os import path, getcwd\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Preperation 3: Moun Drive </h1>\n",
        "\n",
        "> To access data from google drive, We need to give permission to mount the drive and gain access to folders and files"
      ],
      "metadata": {
        "id": "XiqsCfH7q9Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount the google drive used\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2DUnJhipl4Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Preperation 4: File Reference </h1>\n",
        "\n",
        "> Create a dictionary to use for file path reference and for process automation"
      ],
      "metadata": {
        "id": "_IWQb6eara_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPKG_DATASET_DIRECTORY = \"/content/drive/My Drive/COLLEGE_THESIS_FOLDER/GPKG_FILES/\"\n",
        "\n",
        "gpkg_files_dict = {\n",
        "    \"Manila_City\" : \"Manila_City.gpk\",\n",
        "    \"Taguig_City\" : \"Taguig_City.gpk\",\n",
        "    \"Pasig_City\" : \"Pasig_City.gpk\",\n",
        "    \"Paranaque_City\" : \"Paranaque_City.gpk\",\n",
        "    \"Makati_City\" : \"Makati_City.gpk\",\n",
        "    \"LasPinas_City\" : \"LasPinas_City.gpk\",\n",
        "    \"Muntinlupa_City\" : \"Muntinlupa_City.gpk\",\n",
        "    \"Pasay_City\" : \"Pasay_City.gpk\",\n",
        "    \"Malabon_City\" : \"Malabon_City.gpk\",\n",
        "    \"Butuan_City\" : \"Butuan_City.gpk\",\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "DATASET_DIRECTORY = \"/content/drive/My Drive/COLLEGE_THESIS_FOLDER/DataSet/CSV_climactic_factors_dataset/\"\n",
        "\n",
        "CITY_NAME_2011_2020= [\n",
        "    \"Manila_City_2011-2020\",\n",
        "    \"Taguig_City_2011-2020\",\n",
        "    \"Pasig_City_2011-2020\",\n",
        "    \"Paranaque_City_2011-2020\",\n",
        "    \"Makati_City_2011-2020\",\n",
        "    \"Las Pinas_City_2011-2020\",\n",
        "    \"Muntinlupa_City_2011-2020\",\n",
        "    \"Pasay_City_2011-2020\",\n",
        "    \"Malabon_City_2011-2020\",\n",
        "    \"Bututan_City_2011-2020\"\n",
        "]\n",
        "\n",
        "CITY_NAME_2010_2010= [\n",
        "    \"Manila_City_2010-2010\",\n",
        "    \"Taguig_City_2010-2010\",\n",
        "    \"Pasig_City_2010-2010\",\n",
        "    \"Paranaque_City_2010-2010\",\n",
        "    \"Makati_City_2010-2010\",\n",
        "    \"Las Pinas_City_2011-2020\",\n",
        "    \"Muntinlupa_City_2010-2010\",\n",
        "    \"Pasay_City_2010-2010\",\n",
        "    \"Malabon_City_2010-2010\",\n",
        "    \"Bututan_City_2010-2010\"\n",
        "]"
      ],
      "metadata": {
        "id": "cyo_qF39lvu_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Preperation 5: Helper Function / Classes</h1>\n",
        "\n",
        "> We created functions and classes for process that we know we will repeat \n",
        "<br>\n",
        "> For modularity we adapt object oriented programming paradigm\n"
      ],
      "metadata": {
        "id": "XURlSmJjr3z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions and Classes\n",
        "# from t-19 to t-4\n",
        "\n",
        "def get_file_path(city_name_arg):\n",
        "  _fstr = DATASET_DIRECTORY + city_name_arg + \".csv\"\n",
        "  return _fstr\n",
        "\n",
        "def read_and_comb_dataset(ds_n1 = str, ds_n2 = str, row_skip = 13):\n",
        "  df_n1 = filter_data(pd.read_csv(ds_n1, skiprows=row_skip))\n",
        "  df_n2 = filter_data(pd.read_csv(ds_n2, skiprows=row_skip))\n",
        "\n",
        "  df_concat = [df_n1, df_n2]\n",
        "  return pd.concat(df_concat)\n",
        "\n",
        "\n",
        "def filter_data(data_frame_arg):\n",
        "  modif_df = data_frame_arg\n",
        "  year_in_data = np.array(data_frame_arg[\"YEAR\"])\n",
        "\n",
        "  if 2011 in year_in_data and 2010 in year_in_data:\n",
        "    modif_df = data_frame_arg.drop(data_frame_arg[data_frame_arg[\"YEAR\"] == 2011].index)\n",
        "    #print(\"Removed 2011\", 2010 in year_in_data)\n",
        "\n",
        "  elif 2010 in year_in_data and 2020 in year_in_data:\n",
        "    modif_df = data_frame_arg.drop(data_frame_arg[data_frame_arg[\"YEAR\"] == 2010].index)\n",
        "    #print(\"Removed 2010\")\n",
        "\n",
        "  elif 2021 in year_in_data and 2020 in year_in_data:\n",
        "    modif_df = data_frame_arg.drop(data_frame_arg[data_frame_arg[\"YEAR\"] == 2021].index)\n",
        "    #print(\"Removed 2021\")\n",
        "  \n",
        "  return modif_df\n",
        "\n",
        "\n",
        "\n",
        "class ProcessMMreg():\n",
        "  def __init__(self, City_Data, y_val_arg = \"T2M\"):\n",
        "    self.city_data = City_Data\n",
        "    self.y_val = y_val_arg\n",
        "\n",
        "    self.city_df = pd.DataFrame(self.city_data)\n",
        "\n",
        "    self.x = self.city_df.drop(columns = self.y_val)\n",
        "    self.y = self.city_df[self.y_val]\n",
        "\n",
        "    self.lr_model = LinearRegression()\n",
        "\n",
        "  def run_standard_proc(self):\n",
        "    self.split_train_test_data()\n",
        "    self.fit_model()\n",
        "    self.predict_data()\n",
        "    self.get_prediction_plot()\n",
        "\n",
        "  def split_train_test_data(self):\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.x, self.y, test_size = 0.3, random_state = 0)\n",
        "    \n",
        "  def fit_model(self):\n",
        "    self.lr_model.fit(self.X_train, self.y_train)\n",
        "  \n",
        "  def predict_data(self):\n",
        "    self.y_pred_train = self.lr_model.predict(self.X_train)\n",
        "\n",
        "  def get_prediction_plot(self):\n",
        "    plt.scatter(self.y_train, self.y_pred_train)\n",
        "    #plt.xticks(self.y_pred_train)\n",
        "    #plt.yticks(self.y_train)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Bg4kUtWzpXys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Procedure 1: Prepare Data</h1>\n",
        "\n",
        "In this Section we prepare the data by filtering and selecting data that is neccesary to the research"
      ],
      "metadata": {
        "id": "8rC0oguPp2SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a empty dictionary\n",
        "nt_city_df_dict = {}\n",
        "\n",
        "# loop through the list\n",
        "for tf1, tf2 in zip(CITY_NAME_2010_2010, CITY_NAME_2011_2020):\n",
        "  # creating str | expected result ex. Manila_City\n",
        "  tf1_name = tf1.split(\"_\")[0] + \"_\" + tf1.split(\"_\")[1]\n",
        "  tf2_name = tf2.split(\"_\")[0] + \"_\" + tf2.split(\"_\")[1]\n",
        "\n",
        "  # check if combining data from the same city\n",
        "  if tf1_name == tf2_name:\n",
        "    dict_key = tf1_name\n",
        "    # combine data from 2010-2020\n",
        "    comb_df = read_and_comb_dataset(get_file_path(str(tf1)), get_file_path(str(tf2)))\n",
        "\n",
        "    # filter data from t-19 to t-04\n",
        "    nightime_data = comb_df.loc[(comb_df[\"HR\"] <= 4) | (comb_df[\"HR\"] >= 19)]\n",
        "\n",
        "    # create new dictinary item and add new data\n",
        "    nt_city_df_dict[dict_key] = nightime_data\n",
        "\n",
        "  # raise custom error if data are from different city\n",
        "  elif tf1_name != tf2_name:\n",
        "    raise Exception(\"Error: City Names did not match | cannot proceed\")\n",
        "\n",
        "print(nt_city_df_dict.keys())"
      ],
      "metadata": {
        "id": "Pu1kWyc6pas5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# year 2011-2020\n",
        "year_range = [i for i in range(2010, 2021)]\n",
        "# climactic factors\n",
        "df_params = [\"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"]\n",
        "\n",
        "# initialize empty dictionary\n",
        "cities_yparam_avg_val = {}\n",
        "\n",
        "# loop through the cities \n",
        "for city_fname in CITY_NAME_2011_2020: # or CITY_NAME_2010_2010 || Same value\n",
        "  city_dict_key = city_fname.split(\"_\")[0] + \"_\" + city_fname.split(\"_\")[1]\n",
        "  cities_yparam_avg_val[city_dict_key] = {}\n",
        "  \n",
        "  # loop through the climatic factors\n",
        "  for param_name in df_params:\n",
        "    # initialize new empty list every iteration\n",
        "    initial_param_avg_list = []\n",
        "\n",
        "    # loop through the year\n",
        "    for _year in year_range:\n",
        "      # get the average value of the current iterated climatic factor\n",
        "      param_avg_val = np.average(np.array(nt_city_df_dict[city_dict_key].loc[nt_city_df_dict[city_dict_key][\"YEAR\"] == _year][param_name]))\n",
        "      # appent to empty list\n",
        "      initial_param_avg_list.append(param_avg_val)\n",
        "      # add new dictionary key and its value\n",
        "      cities_yparam_avg_val[city_dict_key][param_name] = np.array(initial_param_avg_list)\n",
        "\n",
        "\n",
        "\n",
        "print(cities_yparam_avg_val.keys())"
      ],
      "metadata": {
        "id": "sSrV2XzfpdVG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}